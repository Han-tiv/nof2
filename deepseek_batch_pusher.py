import json
import asyncio
import logging
import aiohttp
from decimal import Decimal
import time
import re
from concurrent.futures import ThreadPoolExecutor
from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_MODEL, DEEPSEEK_URL,
    GEMINI_API_KEY, GEMINI_MODEL, GEMINI_PROJECT,
    AI_PROVIDER
)
from database import redis_client
from volume_stats import (
    calc_volume_compare, get_open_interest, get_funding_rate, get_24hr_change, calc_smart_sentiment,
    get_oi_history, get_top_position_ratio, get_top_account_ratio, get_global_account_ratio
)
from account_positions import account_snapshot, tp_sl_cache
from trend_alignment import calculate_trend_alignment
import google.genai as genai

KEY_REQ = "deepseek_analysis_request_history"
KEY_RES = "deepseek_analysis_response_history"

batch_cache = {}
required_intervals = ["1d", "4h", "1h", "15m", "5m"]

_global_connector = None
_global_session = None
_global_session_lock = asyncio.Lock()

# ================== Session ç®¡ç† ==================
async def close_global_session():
    global _global_session
    async with _global_session_lock:
        if _global_session and not _global_session.closed:
            await _global_session.close()
            _global_session = None
            print("âœ… å…¨å±€ aiohttp session å·²å…³é—­")
            
async def get_global_session():
    global _global_session
    async with _global_session_lock:
        if _global_session is None or _global_session.closed:
            _global_session = aiohttp.ClientSession(
                connector=get_global_connector(),
                timeout=aiohttp.ClientTimeout(total=60)
            )
    return _global_session

def get_global_connector():
    global _global_connector
    if _global_connector is None:
        _global_connector = aiohttp.TCPConnector(
            limit=200,
            limit_per_host=100,
            ttl_dns_cache=300,
            force_close=False
        )
    return _global_connector

def json_safe_dumps(obj):
    return json.dumps(
        obj,
        ensure_ascii=False,
        default=lambda x: float(x) if isinstance(x, Decimal) else str(x)
    )

# ================== Batch ç®¡ç† ==================
def add_to_batch(symbol, interval, klines, indicators):
    if symbol not in batch_cache:
        batch_cache[symbol] = {}
    batch_cache[symbol][interval] = {"klines": klines, "indicators": indicators}

def _is_ready_for_push():
    """
    æ£€æŸ¥ batch_cache æ˜¯å¦æœ‰è‡³å°‘ä¸€ä¸ªå¸ç§æœ‰æ•°æ®ã€‚
    æ”¾å®½è¦æ±‚ï¼Œä¸å†å¼ºåˆ¶æ¯ä¸ªå‘¨æœŸéƒ½å¿…é¡»å®Œæ•´ã€‚
    """
    if not batch_cache:
        print("âš ï¸ batch_cache ä¸ºç©ºï¼Œæ— æ³•æŠ•å–‚")
        return False

    ready_symbols = []
    for symbol, cycles in batch_cache.items():
        if cycles:  # è‡³å°‘æœ‰ä¸€ä¸ªå‘¨æœŸæ•°æ®
            ready_symbols.append(symbol)
        else:
            print(f"âš ï¸ {symbol} ç¼ºå°‘ä»»ä½•å‘¨æœŸæ•°æ®")

    if not ready_symbols:
        print("âš ï¸ æ²¡æœ‰å¸ç§æ»¡è¶³æŠ•å–‚æ¡ä»¶")
        return False

    print(f"âœ… å‡†å¤‡æŠ•å–‚çš„å¸ç§: {ready_symbols}")
    return True

def sentiment_to_signal(score):
    if score >= 85:
        return "ğŸš¨ æç«¯è¿‡çƒ­ | è­¦æƒ•é¡¶éƒ¨åè½¬"
    if score >= 70:
        return "ğŸŸ¢ ç‰›åŠ¿å¼ºåŠ² |"
    if score >= 50:
        return "âšª ä¸­æ€§éœ‡è¡ | è€å¿ƒç­‰å¾…çªç ´"
    if score >= 30:
        return "ğŸŸ¡ ææ…Œç¼“è§£"
    return "ğŸ”¥ æåº¦ææ…Œ"

def _read_prompt():
    try:
        with open("prompt.txt", "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        return "ä½ æ˜¯ä¸€åä¸“ä¸šé‡åŒ–ç­–ç•¥åˆ†æå¼•æ“ï¼Œè¯·ä¸¥æ ¼è¾“å‡º JSON æ•°ç»„æˆ– JSON å¯¹è±¡å½¢å¼çš„äº¤æ˜“ä¿¡å·ã€‚"

# ================== API é¢„åŠ è½½ ==================
async def preload_all_api(dataset):
    results = {
        "funding": {}, "p24": {}, "oi": {}, "sentiment": {},
        "oi_hist": {}, "big_pos": {}, "big_acc": {}, "global_acc": {},
    }

    def safe_call(func, *args, **kwargs):
        try:
            return func(*args, **kwargs)
        except:
            return None

    loop = asyncio.get_running_loop()
    executor = ThreadPoolExecutor(max_workers=20)
    tasks = []

    for symbol, cycles in dataset.items():
        tasks.append(loop.run_in_executor(executor, safe_call, get_funding_rate, symbol))
        tasks.append(loop.run_in_executor(executor, safe_call, get_24hr_change, symbol))
        tasks.append(loop.run_in_executor(executor, safe_call, get_open_interest, symbol))

        for interval in cycles.keys():
            key = f"{symbol}:{interval}"
            tasks.append(loop.run_in_executor(executor, safe_call, get_oi_history, symbol, interval, 10))
            tasks.append(loop.run_in_executor(executor, safe_call, get_top_position_ratio, symbol, interval, 1))
            tasks.append(loop.run_in_executor(executor, safe_call, get_top_account_ratio, symbol, interval, 1))
            tasks.append(loop.run_in_executor(executor, safe_call, get_global_account_ratio, symbol, interval, 1))
            tasks.append(loop.run_in_executor(executor, safe_call, calc_smart_sentiment, symbol, interval))

    completed = await asyncio.gather(*tasks)
    idx = 0
    for symbol, cycles in dataset.items():
        results["funding"][symbol] = completed[idx]; idx += 1
        results["p24"][symbol] = completed[idx]; idx += 1
        results["oi"][symbol] = completed[idx]; idx += 1
        for interval in cycles.keys():
            key = f"{symbol}:{interval}"
            results["oi_hist"][key] = completed[idx]; idx += 1
            results["big_pos"][key] = completed[idx]; idx += 1
            results["big_acc"][key] = completed[idx]; idx += 1
            results["global_acc"][key] = completed[idx]; idx += 1
            results["sentiment"][key] = completed[idx]; idx += 1

    return results

async def preload_all_api_global(dataset_all):
    unified_dataset = {}
    for batch in dataset_all:
        for symbol, cycles in batch.items():
            if symbol not in unified_dataset:
                unified_dataset[symbol] = {}
            for interval, data in cycles.items():
                if interval not in unified_dataset[symbol]:
                    unified_dataset[symbol][interval] = data
    print(f"ğŸ”„ å…¨å±€é¢„åŠ è½½åˆå¹¶äº† {len(unified_dataset)} ä¸ªå¸ç§")
    return await preload_all_api(unified_dataset)

# ================== JSON æå– ==================
def _extract_decision_block(content: str):
    match = re.search(r"<decision>([\s\S]*?)</decision>", content, flags=re.I)
    if not match: return None
    block = match.group(1).strip()
    try:
        parsed = json.loads(block)
        if isinstance(parsed, list): return parsed
    except: pass
    return None

def _extract_all_json(content: str):
    results = []
    try:
        parsed = json.loads(content)
        if isinstance(parsed, list):
            return [x for x in parsed if isinstance(x, dict) and "action" in x]
    except: pass
    matches = re.findall(r'\{[^{}]*\}', content, flags=re.S)
    for m in matches:
        try:
            obj = json.loads(m)
            if isinstance(obj, dict) and "action" in obj:
                results.append(obj)
        except: pass
    return results if results else None

# ================== æŒä»“æ‹†åˆ† ==================
def split_positions_batch(account, dataset_all, max_symbols=2):
    """
    æ‹†åˆ†æŒä»“æ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡åªåŒ…å«ä¸€éƒ¨åˆ†æŒä»“å¸ç§ + positions + balance_info
    æ”¯æŒéƒ¨åˆ†å¸ç§ç¼ºå¤±æ•°æ®
    """
    positions = account.get("positions", [])
    if not positions:
        print("âš ï¸ å½“å‰è´¦æˆ·æ— æŒä»“")
        return []

    balance_info = {
        "balance": account.get("balance"),
        "available": account.get("available"),
        "total_unrealized": account.get("total_unrealized")
    }

    # æŒä»“å¸ç§å¯¹åº”æ•°æ®
    symbol_data = {}
    for p in positions:
        symbol = p["symbol"]
        if symbol in dataset_all and dataset_all[symbol]:  # åªåŠ å…¥æœ‰æ•°æ®çš„å¸ç§
            symbol_data[symbol] = dataset_all[symbol]
        else:
            print(f"âš ï¸ æŒä»“å¸ç§ {symbol} ç¼ºå°‘æ•°æ®ï¼Œå°†è·³è¿‡")

    symbols = list(symbol_data.keys())
    if not symbols:
        print("âš ï¸ æ‰€æœ‰æŒä»“å¸ç§æ•°æ®ç¼ºå¤±ï¼Œè·³è¿‡æŒä»“æ‹†åˆ†")
        return []

    batches = []
    for i in range(0, len(symbols), max_symbols):
        batch_symbols = symbols[i:i+max_symbols]
        batch = {"positions": positions, "balance_info": balance_info}
        for s in batch_symbols:
            batch[s] = symbol_data[s]
        batches.append(batch)

    print(f"âœ… æ‹†åˆ†æŒä»“æ‰¹æ¬¡æ•°é‡: {len(batches)}")
    return batches

# ================== æ‰¹æ¬¡æ‹†åˆ† ==================
def split_dataset_by_symbol_limit(dataset: dict, max_symbols=2):
    """
    æ‹†åˆ†éæŒä»“å¸ç§æ‰¹æ¬¡ï¼Œæ¯æ‰¹æœ€å¤š max_symbols ä¸ªå¸ç§
    æ”¯æŒéƒ¨åˆ†å¸ç§ç¼ºå°‘æ•°æ®
    """
    batches = []

    if "positions" in dataset:
        batches.append({"positions": dataset["positions"], "balance_info": dataset.get("balance_info", {})})

    symbols = [k for k in dataset.keys() if k != "positions"]
    items = [(k, dataset[k]) for k in symbols if dataset[k]]  # åªä¿ç•™æœ‰æ•°æ®çš„å¸ç§

    if not items:
        print("âš ï¸ éæŒä»“å¸ç§æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ‹†åˆ†")
        return batches

    for i in range(0, len(items), max_symbols):
        batch = dict(items[i:i + max_symbols])
        batches.append(batch)

    print(f"âœ… æ‹†åˆ†éæŒä»“æ‰¹æ¬¡æ•°é‡: {len(batches)}")
    return batches

# ================== æ•°æ®æ ¼å¼åŒ– ==================
def _format_dataset(dataset, preloaded=None):
    start_time = time.time()
    text = []
    append = text.append
    account = account_snapshot
    balance_info = dataset.get("balance_info") or {
        "balance": account.get("balance"),
        "available": account.get("available"),
        "total_unrealized": account.get("total_unrealized")
    }

    append("========= ğŸ“Œ å½“å‰è´¦æˆ·èµ„é‡‘çŠ¶æ€ =========")
    append(f"ğŸ’° æ€»æƒç›Š Balance: {round(balance_info.get('balance', 0), 4)}")
    # append(f"ğŸ”“ å¯ç”¨ä½™é¢ Available: {round(balance_info.get('available', 0), 4)}")
    # append(f"ğŸ“‰ æ€»æœªå®ç°ç›ˆäº PnL: {round(balance_info.get('total_unrealized', 0), 4)}")

    # --- åªæ˜¾ç¤ºæ‰¹æ¬¡å†…çš„æŒä»“å¸ ---
    positions = dataset.get("positions", [])
    symbols_in_batch = [k for k in dataset.keys() if k not in ("positions", "balance_info")]
    if positions and symbols_in_batch:
        positions = [p for p in positions if p["symbol"] in symbols_in_batch]

    if positions:
        append("\nğŸ“Œ å½“å‰æŒä»“:")
        for p in positions:
            amt = float(p["size"])
            entry = float(p["entry"])
            mark = float(p["mark_price"])
            pnl = float(p["pnl"])
            side_icon = "ğŸŸ¢ å¤š" if amt > 0 else "ğŸ”´ ç©º"
            pnl_pct = round((mark - entry) / entry * 100, 2) if amt > 0 else round((entry - mark) / entry * 100, 2) if entry > 0 else 0
            line = (
                f"{p['symbol']} | {side_icon} | æ•°é‡ {abs(amt)} | "
                f"å…¥åœº {entry} â†’ å½“å‰ä»·æ ¼ {mark} | ğŸ’µ ç›ˆäº {pnl} ({pnl_pct}%)"
            )
            pos_side = "LONG" if amt > 0 else "SHORT"
            tp_sl_orders = tp_sl_cache.get(p['symbol'], {}).get(pos_side, [])
            if tp_sl_orders:
                tp_sl_lines = [f"{o['type']}={o['stopPrice']}" for o in tp_sl_orders]
                line += " | TP/SL: " + ", ".join(tp_sl_lines)
            else:
                line += " | TP/SL: æ— "
            append(line)
    else:
        append("\nğŸ“Œ å½“å‰æ— æŒä»“")

    # --- éå†æ‰¹æ¬¡å†…å¸ç§ ---
    for symbol in symbols_in_batch:
        cycles = dataset[symbol]
        append(f"\n============ {symbol} å¤šå‘¨æœŸè¡Œæƒ…å¿«ç…§ ============")
        fr     = preloaded.get("funding", {}).get(symbol)
        p24    = preloaded.get("p24", {}).get(symbol)
        oi_now = preloaded.get("oi", {}).get(symbol)
        trend_score = calculate_trend_alignment(cycles)

        if p24:
            append(f"â€¢ 24h æ¶¨è·Œå¹…: {p24['priceChangePercent']}% â†’ æœ€æ–° {p24['lastPrice']} (é«˜ {p24['highPrice']} / ä½ {p24['lowPrice']})")
            append(f"â€¢ 24h æˆäº¤é¢: {round(p24['quoteVolume'] / 1e6, 2)}M USD")
        append(f"ğŸ’° å½“å‰èµ„é‡‘è´¹ç‡ Funding Rate: {fr if fr is not None else 'æœªçŸ¥'}")
        append("\nğŸ“Œ è¶‹åŠ¿ä¸€è‡´æ€§ (Trend Alignment):")
        append(f"ğŸ“ è¶‹åŠ¿æ–¹å‘: {trend_score['TREND_ALIGNMENT_DIRECTION']}")
        append(f"ğŸ“ˆ ç»¼åˆå¾—åˆ†: {trend_score['TREND_ALIGNMENT_SCORE']}/100")
        append(f"ğŸ§© å‘¨æœŸæ˜ç»†: {trend_score['TREND_ALIGNMENT_DETAIL']}")

        for interval in required_intervals:
            if interval not in cycles:
                continue
            data = cycles[interval]
            kl = data["klines"]
            ind = data["indicators"]
            last = kl[-1]
            append(f"\n--- {interval} ---")
            append(f"ğŸ“Œ å½“å‰å‘¨æœŸæ”¶ç›˜ä»·æ ¼: {last['Close']}")

            ema_keys = sorted([k for k in ind.keys() if k.startswith("EMA_") and k[4:].isdigit()], key=lambda x: int(x.split("_")[1]))
            if ema_keys:
                append("\nğŸ“Œ è¶‹åŠ¿æŒ‡æ ‡ï¼ˆEMAï¼‰:")
                for k in ema_keys:
                    append(f"{k}: {round(ind[k], 6)}")
                if "EMA_TREND" in ind:
                    trend = ind["EMA_TREND"]
                    strength = ind.get("EMA_TREND_STRENGTH")
                    append(f"ğŸ“ˆ EMA è¶‹åŠ¿åˆ¤æ–­: {trend}" + (f" | å¼ºåº¦: {round(strength, 4)}" if strength else ""))

            append("\nğŸ“Œ æ³¢åŠ¨ç‡æŒ‡æ ‡:")
            append(f"ATR14: {ind.get('ATR', 'æ•°æ®ä¸è¶³')}")
            append(f"ATR14 20å‘¨æœŸå‡å€¼: {ind.get('ATR_MA20', 'æ•°æ®ä¸è¶³')}")
            append(f"ATRæ¯”ç‡: {ind.get('ATR_RATIO', 'æ•°æ®ä¸è¶³')}")

            key = f"{symbol}:{interval}"
            oi_hist    = preloaded.get("oi_hist", {}).get(key)
            big_pos    = preloaded.get("big_pos", {}).get(key)
            big_acc    = preloaded.get("big_acc", {}).get(key)
            global_acc = preloaded.get("global_acc", {}).get(key)
            sentiment  = preloaded.get("sentiment", {}).get(key)

            append(f"\nğŸ§± å½“å‰æ°¸ç»­æœªå¹³ä»“é‡ OI: {oi_now if oi_now is not None else 'æœªçŸ¥'}")
            if oi_hist: arr = [round(i["openInterest"], 2) for i in oi_hist][-10:]; append(f"â€¢ æœ€æ–°10æ¡å†å² OI æ•°æ®è¶‹åŠ¿: {arr}")
            if big_pos: b = big_pos[-1]; append(f"â€¢ å¤§æˆ·æŒä»“é‡å¤šç©ºæ¯”: {b['ratio']} (å¤š {b['long']}, ç©º {b['short']})")
            if big_acc: b = big_acc[-1]; append(f"â€¢ å¤§æˆ·è´¦æˆ·æ•°å¤šç©ºæ¯”: {b['ratio']} (å¤š {b['long']}, ç©º {b['short']})")
            if global_acc: g = global_acc[-1]; append(f"â€¢ å…¨ç½‘å¤šç©ºäººæ•°æ¯”: {g['ratio']} (å¤š {g['long']}, ç©º {g['short']})")
            if sentiment:
                try:
                    score = sentiment["sentiment_score"]
                    fac = sentiment["factors"]
                    append("\nğŸ“Œ Smart Sentiment Score:")
                    append(f"ğŸ¯ æƒ…ç»ªè¯„åˆ†: {score}/100")
                    append("ğŸ“Š åˆ†é¡¹å› å­(å½’ä¸€åŒ–):")
                    append(f"Â· OIæƒ…ç»ª: {fac['open_interest']}")
                    append(f"Â· Fundingæƒ…ç»ª: {fac['funding_rate']}")
                    append(f"Â· å¤§æˆ·æƒ…ç»ª: {fac['big_whales']}")
                    append(f"Â· æ•£æˆ·åå‘æƒ…ç»ª: {fac['retail_inverse']}")
                    append(f"Â· æˆäº¤é‡æƒ…ç»ª: {fac['volume_emotion']}")
                except Exception:
                    append("\nğŸ“Œ Smart Sentiment Score: è®¡ç®—å¤±è´¥")
            else:
                append("\nğŸ“Œ Smart Sentiment Score: è®¡ç®—å¤±è´¥")

            append("\nğŸ“Œ CVD æŒ‡æ ‡:")
            for k in ["CVD", "CVD_MOM", "CVD_DIVERGENCE", "CVD_PEAKFLIP", "CVD_NORM"]:
                if k in ind:
                    append(f"{k}: {ind[k]}")

            last_buy  = float(last.get("TakerBuyVolume", 0))
            last_sell = float(last.get("TakerSellVolume", 0))
            last_vol  = float(last.get("Volume", 0))
            ratio     = round(last_buy / last_vol * 100, 2) if last_vol > 0 else 0
            append("\nğŸ“Œ ä¸»åŠ¨äº¤æ˜“é‡:")
            append(f"ä¸»åŠ¨ä¹°å…¥é‡(Taker Buy): {last_buy}")
            append(f"ä¸»åŠ¨å–å‡ºé‡(Taker Sell): {last_sell}")
            append(f"ä¸»åŠ¨ä¹°å…¥å æ¯”: {ratio}%")

            vol_info = calc_volume_compare(kl)
            if vol_info:
                append("\nğŸ“Œ æˆäº¤é‡å¯¹æ¯”:")
                append(f"å½“å‰æˆäº¤é‡: {vol_info['current_volume']}")
                append(f"100æ ¹å‡é‡: {vol_info['average_volume_100']}")
                append(f"å½“å‰/å‡é‡æ¯”å€¼: {vol_info['ratio']}")

            opens   = [k["Open"] for k in kl]
            highs   = [k["High"] for k in kl]
            lows    = [k["Low"] for k in kl]
            closes  = [k["Close"] for k in kl]
            volumes = [k["Volume"] for k in kl]
            append("\nğŸ“Œ Kçº¿æ•°ç»„æ ¼å¼ä»æ—§ â†’ æ–°:")
            append(f"open: {opens}")
            append(f"high: {highs}")
            append(f"low: {lows}")
            append(f"close: {closes}")
            append(f"volume: {volumes}")

    append("\nğŸ§  è¯·ç›´æ¥è¾“å‡ºäº¤æ˜“å†³ç­–ï¼Œä¸éœ€è¦æ¨ç†è¿‡ç¨‹ï¼Œåªéœ€JSONæ ¼å¼ï¼š")
    append("æŒ‡ä»¤ï¼šåªè¾“å‡º<decision>æ ‡ç­¾å†…çš„JSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•è§£é‡Šæ–‡å­—ã€‚")
    print(f"[_format_dataset] æ‰§è¡Œè€—æ—¶: {time.time() - start_time:.3f} ç§’")
    return "\n".join(text)

# ================== DeepSeek æ‰¹æ¬¡æŠ•å–‚ ==================
async def _push_single_batch_deepseek(dataset, preloaded, batch_idx, total_batches, session):
    loop = asyncio.get_running_loop()
    formatted_dataset = await loop.run_in_executor(None, _format_dataset, dataset, preloaded)
    system_prompt = await loop.run_in_executor(None, _read_prompt)
    payload = {
        "model": DEEPSEEK_MODEL,
        "messages": [{"role": "system", "content": system_prompt},{"role": "user", "content": formatted_dataset}],
        "temperature": 0.1,
        "max_tokens": 8000,
        "stream": False
    }
    start = time.perf_counter()
    try:
        async with session.post(DEEPSEEK_URL, json=payload, headers={"Authorization": f"Bearer {DEEPSEEK_API_KEY}"}, timeout=aiohttp.ClientTimeout(total=60)) as resp:
            raw = await resp.text()
            print(f"âœ… DeepSeek æ‰¹æ¬¡ {batch_idx} å®Œæˆ | {round((time.perf_counter()-start)*1000,2)} ms | HTTP {resp.status}")
            try:
                root = json.loads(raw)
                content = root["choices"][0]["message"]["content"]
                signals = _extract_decision_block(content) or _extract_all_json(content) or []
            except: signals = []
            return {"batch_idx": batch_idx, "formatted_request": formatted_dataset, "signals": signals, "raw_response": raw, "ts": time.time(), "http_status": resp.status}
    except Exception as e:
        logging.error(f"âŒ DeepSeek æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
        return {"batch_idx": batch_idx, "formatted_request": formatted_dataset, "signals": [], "raw_response": str(e), "ts": time.time(), "http_status": None, "error": str(e)}

# ================== Gemini æ‰¹æ¬¡æŠ•å–‚ ==================
async def _push_single_batch_gemini(dataset, preloaded, batch_idx, total_batches):
    loop = asyncio.get_running_loop()
    formatted_dataset = await loop.run_in_executor(None, _format_dataset, dataset, preloaded)
    system_prompt = await loop.run_in_executor(None, _read_prompt)

    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": formatted_dataset}
        ]

        # ä½¿ç”¨ generate_content è€Œé chat.create
        resp = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=system_prompt + "\n\n" + formatted_dataset,  # åˆå¹¶æˆå•æ¡å­—ç¬¦ä¸²
            config={
                "temperature": 0.1,
                "max_output_tokens": 8000
            }
        )
        content = resp.text
        signals = _extract_decision_block(content) or _extract_all_json(content) or []

        print(f"âœ… Gemini æ‰¹æ¬¡ {batch_idx}/{total_batches} å®Œæˆ | HTTP 200")
        return {
            "batch_idx": batch_idx,
            "formatted_request": formatted_dataset,
            "signals": signals,
            "raw_response": content,
            "ts": time.time(),
            "http_status": 200
        }

    except Exception as e:
        return {
            "batch_idx": batch_idx,
            "formatted_request": formatted_dataset,
            "signals": [],
            "raw_response": str(e),
            "ts": time.time(),
            "http_status": None,
            "error": str(e)
        }

# ================== é€šç”¨æ‰¹é‡æŠ•å–‚ ==================
async def push_batch_to_ai():
    if not _is_ready_for_push():
        return None

    start_total = time.perf_counter()  # è®°å½•æ€»å¼€å§‹æ—¶é—´

    dataset_all = batch_cache.copy()
    batch_cache.clear()
    all_signals = []

    account = account_snapshot

    # --- 1. æ‹†åˆ†æŒä»“æ‰¹æ¬¡ ---
    positions_batches = split_positions_batch(account, dataset_all, max_symbols=2)
    positions_symbols = [p["symbol"] for batch in positions_batches for p in batch.get("positions", [])]

    # --- 2. æ‹†åˆ†éæŒä»“å¸ç§ ---
    symbol_dataset = {k: v for k, v in dataset_all.items() if k not in positions_symbols}
    symbol_batches = split_dataset_by_symbol_limit(symbol_dataset, max_symbols=2)

    # --- 3. åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ ---
    batches = positions_batches + symbol_batches

    # --- 4. é¢„åŠ è½½ï¼Œåªé’ˆå¯¹æ‰¹æ¬¡é‡Œçš„å¸ç§ ---
    preloaded_batches = []
    for batch in batches:
        symbols_only = {k: v for k, v in batch.items() if k not in ("positions", "balance_info")}
        preloaded = await preload_all_api(symbols_only) if symbols_only else {}
        preloaded_batches.append(preloaded)

    # --- 5. åˆ›å»ºæŠ•å–‚ä»»åŠ¡ ---
    tasks = []
    for idx, batch in enumerate(batches):
        preloaded = preloaded_batches[idx]
        if AI_PROVIDER == "deepseek":
            session = await get_global_session()
            tasks.append(_push_single_batch_deepseek(batch, preloaded, idx+1, len(batches), session))
        else:
            tasks.append(_push_single_batch_gemini(batch, preloaded, idx+1, len(batches)))

    # --- 6. æ‰§è¡ŒæŠ•å–‚ ---
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # --- 7. ä¿å­˜è¯·æ±‚/å“åº”åˆ° Redisï¼Œå¹¶æ±‡æ€»ä¿¡å· ---
    for r in results:
        if not isinstance(r, dict):
            continue
        if r.get("formatted_request"):
            redis_client.rpush(KEY_REQ, json_safe_dumps({
                "batch_idx": r["batch_idx"],
                "request": r["formatted_request"],
                "timestamp": r["ts"]
            }))
        redis_client.rpush(KEY_RES, json_safe_dumps({
            "batch_idx": r["batch_idx"],
            "signals": r.get("signals", []),
            "raw_response": r.get("raw_response"),
            "timestamp": r["ts"],
            "http_status": r.get("http_status"),
            "error": r.get("error")
        }))
        all_signals.extend(r.get("signals", []))

    end_total = time.perf_counter()
    print(f"ğŸ“Š è¯·æ±‚ç»Ÿè®¡: æŠ•å–‚æ‰¹æ¬¡ {len(results)} | æ€»è€—æ—¶ {round((end_total - start_total), 2)} ç§’")

    return all_signals if all_signals else None

# åˆ«åä¿ç•™
push_batch_to_deepseek = push_batch_to_ai